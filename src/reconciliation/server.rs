use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::Semaphore;
use tokio_util::sync::CancellationToken;
use crate::{app_error, app_info, app_warn};
use crate::baseline::HttpClient;
use crate::clickhouse::clickhouse::ClickHouse;
use crate::database::postgresql::DatabaseConnection;
use crate::database::repositories::AtomicityData;
use crate::reconciliation::model::{AppConfig, ReconciliationServer};
use crate::database::repositories::mint_stats::MintStatsRepository;
use crate::database::repositories::reconciliation_schedule::ReconciliationScheduleRepository;
use crate::error::{ReconciliationError, Result};


/// å¯¹è´¦è¿è¡Œæ—¶é…ç½®
#[derive(Debug, Clone)]
struct ReconciliationConfig {
    max_concurrent: usize,
    max_difference_percent: u64,
    timeout_seconds: u64,
}

impl ReconciliationConfig {
    fn from_env() -> Self {
        let max_concurrent = std::env::var("RECONCILIATION_MAX_CONCURRENT")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(10);

        let max_difference_percent = std::env::var("RECONCILIATION_MAX_DIFFERENCE")
            .ok()
            .and_then(|v| v.parse::<u64>().ok())
            .unwrap_or(10);

        let timeout_seconds = std::env::var("RECONCILIATION_TIMEOUT_SECONDS")
            .ok()
            .and_then(|v| v.parse::<u64>().ok())
            .unwrap_or(300);

        Self {
            max_concurrent,
            max_difference_percent,
            timeout_seconds,
        }
    }
}

/// å•ä¸ªå¯¹è´¦ä»»åŠ¡çš„ç»“æœ
#[derive(Debug)]
struct ReconciliationTaskResult {
    mint_pubkey: String,
    db_holder_count: i64,
    last_holder_count: i64,
    onchain_result: Result<u64>,
}

impl ReconciliationServer {
    pub fn new(
        database: Arc<DatabaseConnection>,
        clickhouse: Arc<ClickHouse>,
        http_client: Arc<HttpClient>
    ) -> Result<Self> {
        let settings = config::Config::builder()
            .add_source(config::File::with_name("config/default"))
            // ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¦†ç›–
            .add_source(config::Environment::with_prefix("APP"))
            .build()
            .map_err(|e| ReconciliationError::ServerCreationFailed(
                format!("Failed to read config: {}", e)
            ))?;

        let app_config = settings.try_deserialize::<AppConfig>()
            .map_err(|e| ReconciliationError::ServerCreationFailed(
                format!("Failed to read config: {}", e)
            ))?;

        Ok(Self{
            app_config,
            database,
            clickhouse,
            http_client,
        })
    }

    pub async fn start_with_cancellation(
        &self,
        cancellation_token: CancellationToken,
    ) -> Result<()> {
        // åŠ è½½é…ç½®
        let config = ReconciliationConfig::from_env();

        app_info!(
            "ğŸ”§ Reconciliation é…ç½®: max_concurrent={}, max_difference={}%, timeout={}s",
            config.max_concurrent, config.max_difference_percent, config.timeout_seconds
        );

        loop {
            tokio::select! {
                _ = cancellation_token.cancelled() => {
                    app_info!("reconciliation server received cancellation signal. Shutting down...");
                    break;
                }
                data = self.database.get_due_mints(self.app_config.dues_batch_size) => {
                    match data {
                        Ok(due_mints) if !due_mints.is_empty() => {
                            // å¤„ç†è¿™æ‰¹å¯¹è´¦ä»»åŠ¡
                            if let Err(e) = self.process_reconciliation_batch(due_mints, &config).await {
                                app_error!("Failed to process reconciliation batch: {}", e);
                            }
                        }
                        Ok(_) => {
                            // æ²¡æœ‰éœ€è¦å¯¹è´¦çš„ mintï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åç»§ç»­
                            tokio::time::sleep(tokio::time::Duration::from_secs(60)).await;
                        }
                        Err(err) => {
                            app_error!("Failed to get due mints: {:?}", err);
                            // ä¸ç›´æ¥è¿”å›é”™è¯¯ï¼Œè€Œæ˜¯ç­‰å¾…åé‡è¯•
                            tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;
                        }
                    }
                }
            }
        }
        Ok(())
    }

    fn calculate_change_percentage(current: i64, last: i64) -> f64 {
        if last > 0 {
            (current - last).abs() as f64 / last as f64 * 100.0
        } else {
            0.0
        }
    }

    fn determine_next_interval(change_percentage: f64, config: &AppConfig) -> i32 {
        for tier in &config.scheduling_tiers {
            if change_percentage >= tier.threshold_percent {
                return tier.interval_hours;
            }
        }
        config.default_interval_hours
    }

    /// æ ¹æ®æ•°æ®åº“æŒæœ‰è€…æ•°é‡å˜åŒ–æ›´æ–°å¯¹è´¦è®¡åˆ’
    async fn update_schedule_based_on_db_change(
        &self,
        mint_pubkey: &str,
        current_db_count: i64,
        last_holder_count: i64,
    ) -> Result<i32> {
        let change_percentage = Self::calculate_change_percentage(
            current_db_count,
            last_holder_count
        );

        let next_interval_hours = Self::determine_next_interval(
            change_percentage,
            &self.app_config
        );

        self.database
            .update_schedule_after_reconciliation(
                mint_pubkey,
                current_db_count,
                next_interval_hours
            )
            .await?;

        Ok(next_interval_hours)
    }

    /// æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ holder counts
    async fn fetch_db_holder_counts(
        &self,
        mint_pubkeys: &[String],
    ) -> Result<HashMap<String, i64>> {
        let db_holder_counts = self.database.get_holder_counts_batch(mint_pubkeys).await?;
        Ok(db_holder_counts.into_iter().collect())
    }

    /// å¹¶å‘è·å–é“¾ä¸Šæ•°æ®
    async fn fetch_onchain_data_concurrently(
        &self,
        due_mints: Vec<crate::database::repositories::reconciliation_schedule::ReconciliationSchedule>,
        db_counts_map: &HashMap<String, i64>,
        config: &ReconciliationConfig,
    ) -> Vec<std::result::Result<ReconciliationTaskResult, tokio::task::JoinError>> {
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent));
        let mut handles = Vec::new();

        for schedule in due_mints {
            let permit = match semaphore.clone().acquire_owned().await {
                Ok(p) => p,
                Err(e) => {
                    app_error!("Failed to acquire semaphore: {}", e);
                    continue;
                }
            };

            let http_client = self.http_client.clone();
            let mint_pubkey = schedule.mint_pubkey.clone();
            let current_db_count = db_counts_map.get(&mint_pubkey).copied().unwrap_or(0);
            let last_holder_count = schedule.last_holder_count;

            let handle = tokio::spawn(async move {
                let _permit = permit;

                // todo!: ç®—äº†ï¼Œæš‚æ—¶å…ˆç›´æ¥é€šè¿‡apiè·å–å§ï¼Œåç»­åœ¨è¿›è¡Œå…¶ä»–çš„è€ƒè™‘
                // let onchain_result = http_client.get_token_holders(&mint_pubkey).await;
                let onchain_result = http_client.get_sol_scan_holder(&mint_pubkey).await;

                ReconciliationTaskResult {
                    mint_pubkey,
                    db_holder_count: current_db_count,
                    last_holder_count,
                    onchain_result,
                }
            });

            handles.push(handle);
        }

        futures::future::join_all(handles).await
    }

    /// åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»º
    fn should_rebuild(
        &self,
        db_count: i64,
        onchain_count: i64,
        config: &ReconciliationConfig,
    ) -> bool {
        if db_count == 0 {
            // æ•°æ®åº“ä¸ºç©ºä½†é“¾ä¸Šæœ‰ holderï¼Œéœ€è¦é‡å»º
            return onchain_count > 0;
        }

        // æ­£å¸¸è®¡ç®—å·®å¼‚ç™¾åˆ†æ¯”
        let difference = (db_count - onchain_count).abs();
        let difference_percentage = difference * 100 / db_count;
        difference_percentage > config.max_difference_percent as i64
    }

    /// æ‰§è¡Œé‡å»ºå’Œè¿½èµ¶æµç¨‹
    async fn rebuild_and_catchup(&self, mint_pubkey: &str) -> Result<()> {
        app_info!("Starting rebuild for mint {} due to significant difference", mint_pubkey);

        // é‡æ–°è·å–å®Œæ•´çš„ holder åˆ—è¡¨ç”¨äºé‡å»º
        let holders = self.http_client.get_token_holders(mint_pubkey).await?;

        if holders.is_empty() {
            app_warn!("Got empty holders list for mint {}, skipping rebuild", mint_pubkey);
            return Ok(());
        }

        // å»ºç«‹ baseline
        let baseline_slot = self.database
            .establish_baseline_atomic(mint_pubkey, holders)
            .await?;

        app_info!("Rebuilt baseline for mint {} at slot {}", mint_pubkey, baseline_slot);

        // æ‰§è¡Œ catch-up
        self.catch_up(baseline_slot, mint_pubkey).await?;

        app_info!("âœ… Successfully rebuilt and caught up for mint {}", mint_pubkey);
        Ok(())
    }

    /// å¤„ç†å•ä¸ªå¯¹è´¦ç»“æœ
    async fn process_single_reconciliation_result(
        &self,
        task_result: ReconciliationTaskResult,
        config: &ReconciliationConfig,
    ) -> Result<()> {
        match task_result.onchain_result {
            Ok(holders_count) => {
                let holders_count = holders_count as i64;

                // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
                let needs_rebuild = self.should_rebuild(
                    task_result.db_holder_count,
                    holders_count,
                    config,
                );

                if needs_rebuild {
                    let difference = (task_result.db_holder_count - holders_count).abs();

                    if task_result.db_holder_count == 0 {
                        app_warn!(
                            "Token:{} has {} holders onchain but 0 in db, needs rebuild",
                            &task_result.mint_pubkey,
                            holders_count
                        );
                    } else {
                        let difference_percentage = difference * 100 / task_result.db_holder_count;
                        app_error!(
                            "Token:{} count in db {} is not same as onchain count {}, difference: {}%",
                            &task_result.mint_pubkey,
                            task_result.db_holder_count,
                            holders_count,
                            difference_percentage
                        );
                    }

                    // æ‰§è¡Œé‡å»ºæµç¨‹
                    if let Err(e) = self.rebuild_and_catchup(&task_result.mint_pubkey).await {
                        app_error!("Failed to rebuild mint {}: {}", task_result.mint_pubkey, e);
                    }
                }

                // æ ¹æ®æ•°æ®åº“æŒæœ‰è€…æ•°é‡å˜åŒ–æ›´æ–°å¯¹è´¦è®¡åˆ’
                match self.update_schedule_based_on_db_change(
                    &task_result.mint_pubkey,
                    task_result.db_holder_count,
                    task_result.last_holder_count,
                ).await {
                    Ok(next_interval_hours) => {
                        app_info!(
                            "âœ… Reconciliation completed for mint {}: onchain={}, db={}, next_interval={}h",
                            task_result.mint_pubkey,
                            holders_count,
                            task_result.db_holder_count,
                            next_interval_hours
                        );
                    }
                    Err(e) => {
                        app_error!("Failed to update schedule for mint {}: {}", task_result.mint_pubkey, e);
                    }
                }
            }
            Err(e) => {
                // RPC è°ƒç”¨å¤±è´¥ï¼Œä»ç„¶æ›´æ–° scheduleï¼ˆåŸºäºæ•°æ®åº“å˜åŒ–ï¼‰
                app_warn!("Failed to get onchain data for mint {}: {}", task_result.mint_pubkey, e);

                if let Err(e) = self.update_schedule_based_on_db_change(
                    &task_result.mint_pubkey,
                    task_result.db_holder_count,
                    task_result.last_holder_count,
                ).await {
                    app_error!("Failed to update schedule for mint {}: {}", task_result.mint_pubkey, e);
                }
            }
        }

        Ok(())
    }

    /// å¤„ç†ä¸€æ‰¹å¯¹è´¦ä»»åŠ¡
    async fn process_reconciliation_batch(
        &self,
        due_mints: Vec<crate::database::repositories::reconciliation_schedule::ReconciliationSchedule>,
        config: &ReconciliationConfig,
    ) -> Result<()> {
        let total_mints = due_mints.len();
        app_info!("Found {} mints due for reconciliation", total_mints);

        // 1. æå–æ‰€æœ‰ mint_pubkeys
        let mint_pubkeys: Vec<String> = due_mints
            .iter()
            .map(|schedule| schedule.mint_pubkey.clone())
            .collect();

        // 2. æ‰¹é‡æŸ¥è¯¢å½“å‰æ•°æ®åº“ä¸­çš„ holder_count
        let db_counts_map = self.fetch_db_holder_counts(&mint_pubkeys).await?;

        // 3. å¹¶å‘è·å–é“¾ä¸Šæ•°æ®
        app_info!("Waiting for all reconciliation tasks to complete (timeout: {}s)", config.timeout_seconds);

        let timeout_duration = tokio::time::Duration::from_secs(config.timeout_seconds);
        let results = match tokio::time::timeout(
            timeout_duration,
            self.fetch_onchain_data_concurrently(due_mints, &db_counts_map, config)
        ).await {
            Ok(results) => results,
            Err(_) => {
                app_error!(
                    "Reconciliation batch timeout after {}s for {} mints, skipping this batch",
                    config.timeout_seconds,
                    total_mints
                );
                return Ok(());
            }
        };

        // 4. å¤„ç†ç»“æœ
        for result in results {
            match result {
                Ok(task_result) => {
                    if let Err(e) = self.process_single_reconciliation_result(task_result, config).await {
                        app_error!("Failed to process reconciliation result: {}", e);
                    }
                }
                Err(e) => {
                    app_error!("Task panicked: {}", e);
                }
            }
        }

        Ok(())
    }

    /// ä» baseline_slot è¿½èµ¶åˆ°å½“å‰å·²æœ‰çš„å†å²äº‹ä»¶
    /// å½“ next_cursor ä¸º None æ—¶è¡¨ç¤ºå†å²æ•°æ®å·²è¿½å®Œï¼Œç›´æ¥é€€å‡º
    /// ä¹‹åçš„æ–°äº‹ä»¶ç”± consume_events_from_queue ç»Ÿä¸€å¤„ç†
    pub async fn catch_up(&self, baseline_slot: i64, mint: &str) -> Result<()> {
        const BATCH_SIZE: i64 = 1000;
        let mut cursor = (baseline_slot - 1, i64::MAX);
        let mut total_processed = 0;

        app_info!(
            "Starting catch-up for mint {} from slot {}",
            mint, baseline_slot
        );

        // åœ¨å¼€å§‹å¤„ç†ä¹‹å‰ï¼Œå°† baseline_slot ä¹‹å‰çš„æ‰€æœ‰æœªç¡®è®¤äº‹ä»¶æ ‡è®°ä¸º confirmed
        // å› ä¸º baseline å·²ç»ä»£è¡¨äº†é‚£ä¸ªæ—¶åˆ»çš„å®Œæ•´çŠ¶æ€ï¼Œè¿™äº›è¿‡æ—¶çš„äº‹ä»¶ä¸éœ€è¦å†å¤„ç†
        let skipped_count = self
            .clickhouse
            .confirm_events_before_baseline(mint, baseline_slot)
            .await?;
        if skipped_count > 0 {
            app_info!(
                "Skipped {} events before baseline_slot {} for mint {}",
                skipped_count, baseline_slot, mint
            );
        }

        loop {
            match self
                .clickhouse
                .get_next_events_batch(cursor, mint, BATCH_SIZE)
                .await
            {
                Ok((token_events, Some(next_cursor))) => {
                    // æœ‰æ›´å¤šå†å²æ•°æ®ï¼Œç»§ç»­å¤„ç†
                    if token_events.is_empty() {
                        cursor = next_cursor;
                        continue;
                    }

                    app_info!("In next_cursor to sync mint atomic");
                    self.database
                        .upsert_synced_mints_atomic(&token_events, &self.clickhouse)
                        .await?;

                    total_processed += token_events.len();
                    cursor = next_cursor;
                }
                Ok((token_events, None)) => {
                    // æ²¡æœ‰æ›´å¤šå†å²æ•°æ®ï¼Œå¤„ç†æœ€åä¸€æ‰¹åé€€å‡º
                    app_info!("In none to sync mint atomic");
                    if !token_events.is_empty() {
                        self.database
                            .upsert_synced_mints_atomic(&token_events, &self.clickhouse)
                            .await?;
                        total_processed += token_events.len();
                    }

                    app_info!(
                        "Catch-up completed for mint {}: processed {} events",
                        mint, total_processed
                    );
                    break;
                }
                Err(e) => {
                    app_error!("Failed to get events batch for mint {}: {}", mint, e);
                    return Err(e);
                }
            }
        }

        Ok(())
    }
}