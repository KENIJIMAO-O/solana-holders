use rust_decimal::Decimal;
use std::collections::HashSet;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::Semaphore;
use tokio_util::sync::CancellationToken;
use crate::database::postgresql::DatabaseConnection;
use crate::database::repositories::AtomicityData;
use crate::database::repositories::tracked_mints::TrackedMintsRepository;
use crate::baseline::HttpClient;
use crate::{app_error, app_info, BIG_TOKEN_HOLDER_COUNT};
use crate::clickhouse::clickhouse::{ClickHouse, Event};
use crate::database::repositories::mint_stats::MintStatsRepository;
use crate::kafka::KafkaMessageQueue;
use crate::error::Result;

#[derive(Clone)]
pub struct SyncController {
    pub kafka_queue: Arc<KafkaMessageQueue>,
    pub database: Arc<DatabaseConnection>,
    pub clickhouse: Arc<ClickHouse>,
    pub http_client: Arc<HttpClient>,
}

impl SyncController {
    pub fn new(
        kafka_queue: Arc<KafkaMessageQueue>,
        database: Arc<DatabaseConnection>,
        clickhouse: Arc<ClickHouse>,
        http_client: Arc<HttpClient>,
    ) -> Self {
        Self {
            kafka_queue,
            database,
            clickhouse,
            http_client,
        }
    }

    pub async fn consume_events_from_queue(
        &self,
        cancellation_token: CancellationToken,
    ) -> Result<()> {
        // ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        let batch_size = std::env::var("TOKEN_EVENT_BATCH_SIZE")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(1000);

        let batch_timeout_ms = std::env::var("TOKEN_EVENT_BATCH_TIMEOUT_MS")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(100);

        let max_consecutive_failures = std::env::var("TOKEN_EVENT_MAX_FAILURES")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(10);

        let consumer_name = "token_event_dequeuer";

        app_info!(
            "ğŸ”§ Token Event Consumer é…ç½®: batch_size={}, timeout={}ms, max_failures={}",
            batch_size, batch_timeout_ms, max_consecutive_failures
        );

        'retry_loop: loop {
            let mut consecutive_failures = 0;

            loop{
                tokio::select! {
                _ = cancellation_token.cancelled() => {
                    app_info!("Monitor received cancellation signal. Shutting down...");
                    break 'retry_loop;
                }

                datas_result = self.kafka_queue.batch_dequeue_holder_event(
                    consumer_name,
                    batch_size,
                    batch_timeout_ms as usize,
                ) => {
                    let datas = match datas_result {
                        Ok(d) => {
                            consecutive_failures = 0;
                            d
                        },
                        Err(e) => {
                            consecutive_failures += 1;
                            if consecutive_failures >= max_consecutive_failures {
                                app_error!("Failed to dequeue {} times in a row. Restarting consumer.", max_consecutive_failures);
                                break;
                            }
                            app_error!("Failed to dequeue from Redis: {}", e);
                            tokio::time::sleep(Duration::from_secs(1)).await;
                            continue; // ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
                        }
                    };

                    // å¦‚æœä¸ºç©ºï¼Œè¯´æ˜æ•°æ®è¿˜æ²¡è¿›é˜Ÿ
                    if datas.is_empty() {
                        tokio::time::sleep(Duration::from_millis(10)).await;
                        continue;
                    }

                    app_info!("batch dequeue holder events complete");

                    // --- æ•°æ®æ¸…æ´—ã€è½¬æ¢å’Œèšåˆï¼ˆå•æ¬¡è¿­ä»£ï¼‰---
                    let capacity = datas.len();
                    let mut _message_ids = Vec::with_capacity(capacity);
                    let mut token_events = Vec::with_capacity(capacity);
                    let mut unique_mints = HashSet::new();

                    for (message_id, raw_event) in datas {
                        match raw_event.delta.parse::<Decimal>() {
                            Ok(delta) => {
                                let confirmed_u8 = if raw_event.confirmed { 1u8 } else { 0u8 };
                                let event = Event::new(
                                    raw_event.slot,
                                    raw_event.tx_signature, // move
                                    raw_event.mint_address.to_string(),
                                    raw_event.account_address.to_string(),
                                    raw_event.owner_address.map_or("".to_string(), |o| o.to_string()),
                                    delta,
                                    confirmed_u8,
                                );
                                unique_mints.insert(event.mint_pubkey.clone()); // Clone for the set
                                token_events.push(event); // Move event
                                _message_ids.push(message_id); // Move message_id
                            }
                            Err(e) => {
                                // todo!: è¿™é‡Œå…¶å®ä¹Ÿæœ‰é—®é¢˜ï¼Œå› ä¸ºç›´æ¥ä¸¢å¼ƒä¸€ä¸ªäº‹ä»¶çš„è¯ï¼Œå…¶å®å¾ˆæœ‰å¯èƒ½å¯¼è‡´ç›¸å…³ä»£å¸åç»­æ‰€æœ‰ä¿¡æ¯æ›´æ–°å…¨é”™
                                app_error!(
                                    "Skipping event with invalid delta. Tx: {}, Delta: '{}', Error: {}",
                                    raw_event.tx_signature, raw_event.delta, e
                                );
                            }
                        }
                    }

                    // ä» HashSet åˆ›å»ºæœ€ç»ˆçš„ mints Vec
                    let mints: Vec<String> = unique_mints.into_iter().collect();

                    let untracked_mints = self.database.is_tracked_batch(&mints).await?;
                    app_info!("{}", &format!("untracked_mints_len:{}", untracked_mints.len()));

                    self.kafka_queue.batch_enqueue_baseline_task(&untracked_mints).await?;
                    app_info!("complete batch enqueue baseline");

                    // --- æ ¸å¿ƒèŒè´£ï¼šå°†æ–°çš„æ•°æ®æ›´æ–°åˆ°æ•°æ®åº“ä¸­ ---
                    // è¿™é‡Œæˆ‘æƒ³è¯´çš„å°±æ˜¯ï¼Œå¯¹äºä»»æ„ä¸€ä¸ªæ•°æ®ï¼Œä¸€å®šä¼šè¿›eventsè¡¨ï¼Œå¦‚æœè¿™ä¸ªä»£å¸å·²ç»æ„å»ºäº†baselineï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥åˆ©ç”¨ä»token queueè·å–çš„æ•°æ®æ›´æ–°
                    // å¦‚æœæ²¡æœ‰æ„å»ºbaselineï¼Œé‚£ä¹ˆå°±ä¸ç”¨æ›´æ–°ï¼Œç­‰åˆ°æ„å»ºå®Œä¹‹åï¼Œcatch-upéœ€è¦ä»æ•°æ®ä¸­å°†æ‰€æœ‰å’Œä»–ç›¸å…³çš„eventså…¨éƒ¨åˆå¹¶ä¹‹åï¼Œå›åˆ°token queue
                    // æ‰€ä»¥å¯¹äºæ•°æ®åº“ä¸­çš„æ•°æ®éœ€è¦æ›´æ–°ä¸‰æ¬¡ï¼Œç¬¬ä¸€æ˜¯baselineæ„å»ºçš„æ—¶å€™çš„æ›´æ–°ï¼Œç¬¬äºŒæ˜¯catch-upæ—¶å€™çš„æ›´æ–°ï¼Œæœ€åæ˜¯å½“å‰å‡½æ•°ä¸­token queueçš„æ›´æ–°
                    if !token_events.is_empty() {

                        // å¯¹äºeventsè¡¨ï¼Œæ— è®ºå½“å‰ä»£å¸å¤„äº Not_started baseline_building catching_up synced ä»»æ„ä¸€ä¸ªé˜¶æ®µï¼Œéƒ½éœ€è¦æ›´æ–°
                        match self.clickhouse.upsert_events_batch(&token_events).await {
                            Ok(()) => {
                                // ack token_queue message
                                 if let Err(e) = self.kafka_queue.ack_token_events(consumer_name).await {
                                    app_error!("Error acknowledging messages: {}", e);
                                    // ACK å¤±è´¥æ˜¯ä¸€ä¸ªä¸¥é‡é—®é¢˜ï¼Œéœ€è¦è€ƒè™‘å¦‚ä½•å¤„ç†ï¼ˆé‡è¯•æˆ–å‘Šè­¦ï¼‰
                                    continue;
                                };
                            } ,
                            Err(e) => {
                                app_error!("Error upserting events: {}", e);
                                // å¦‚æœå†™å…¥æ•°æ®åº“å¤±è´¥ï¼Œæˆ‘ä»¬ä¸åº”è¯¥ ACK æ¶ˆæ¯ï¼Œè®©å®ƒå¯ä»¥è¢«é‡æ–°å¤„ç†
                                continue;
                            }
                        };
                        app_info!("sql upsert events complete");

                        // å¯¹äºå…¶ä»–çš„å‡ ä¸ªè¡¨ï¼Œå¿…é¡»ç­‰åˆ°ä»£å¸å®Œæˆcatch-upä¹‹åï¼Œå³tracked_mints.status == synced æ‰èƒ½åœ¨è¿™é‡Œæ›´æ–°
                        let synced_mints = self.database.filter_synced_mints(&mints).await?;
                        let synced_mints_set: HashSet<&str> = synced_mints.iter().map(|s| s.as_str()).collect();

                        let synced_token_events: Vec<Event> = token_events
                            .into_iter()
                            .filter(|token_event| {
                                synced_mints_set.contains(token_event.mint_pubkey.as_str())
                            })
                            .collect();

                        if synced_token_events.is_empty() { continue }

                        // è¿™ä¿©å¯èƒ½éœ€è¦ç»‘å®šåœ¨ä¸€å—
                        if let Err(e) = self.database.upsert_synced_mints_atomic(&synced_token_events, &self.clickhouse).await {
                            app_error!("upsert token_account, holders, mint_stats error: {}", e);
                        }

                        app_info!("sql upsert token_account, holders, mint_stats complete");
                        }
                    }
                }
            }
            app_error!("Event consumer loop failed. Retrying in 10 seconds...");
            tokio::time::sleep(Duration::from_secs(10)).await;
        }

        Ok(())
    }

    pub async fn consume_baseline_mints_for_queue(
        &self,
        cancellation_token: CancellationToken,
    ) -> Result<()> {
        // ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        let max_concurrent = std::env::var("BASELINE_MAX_CONCURRENT")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(3);

        let max_tasks_in_memory = std::env::var("BASELINE_MAX_TASKS_MEMORY")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(10);

        let dequeue_size = std::env::var("BASELINE_DEQUEUE_SIZE")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(3);

        let batch_timeout_ms = std::env::var("BASELINE_BATCH_TIMEOUT_MS")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(100);

        let max_consecutive_failures = std::env::var("BASELINE_MAX_FAILURES")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(10);

        let consumer_name = "baseline_dequeuer";

        app_info!(
            "ğŸ”§ Baseline Consumer é…ç½®: max_concurrent={}, max_tasks_memory={}, dequeue_size={}, timeout={}ms, max_failures={}",
            max_concurrent, max_tasks_in_memory, dequeue_size, batch_timeout_ms, max_consecutive_failures
        );

        // execution_semaphore: æ§åˆ¶åŒæ—¶æ‰§è¡Œçš„ä»»åŠ¡æ•°
        let execution_semaphore = Arc::new(Semaphore::new(max_concurrent));
        // memory_semaphore: æ§åˆ¶å†…å­˜ä¸­çš„ä»»åŠ¡æ€»æ•°
        // å¦‚æœä¸è¦å†…å­˜æ§åˆ¶ä¼šå¯¼è‡´loopä¸€ç›´å‡ºé˜Ÿï¼Œè°ƒåº¦tokio::spawnï¼Œè™½ç„¶è¿™äº›ä»»åŠ¡ä¼šå› ä¸ºexecution_semaphoreçš„å­˜åœ¨ä¸ä¼šåŒæ—¶æ‰§è¡Œï¼Œä½†æ˜¯ä¸€æ ·ä¼šå¯¼è‡´å†…å­˜æ— é™åˆ¶çš„å¢åŠ 
        let memory_semaphore = Arc::new(Semaphore::new(max_tasks_in_memory));

        tokio::time::sleep(Duration::from_secs(10)).await; // ç­‰å¾…æ¶ˆæ¯é˜Ÿåˆ—ä¸­æœ‰ä¸€äº›å€¼

        'retry_loop: loop {
            let mut consecutive_failures = 0;

            loop {
                tokio::select! {
                    // åˆ†æ”¯1 æ”¶åˆ°äº†å–æ¶ˆä¿¡æ¯
                    _ = cancellation_token.cancelled() => {
                        app_info!("baseline consumer received cancellation signal. Shutting down...");
                        break 'retry_loop;
                    }

                    mints_result = self.kafka_queue.batch_dequeue_baseline_task(
                        consumer_name,
                        dequeue_size,
                        batch_timeout_ms as usize,
                    ) => {
                        let mints = match mints_result {
                            Ok(m) => {
                                consecutive_failures = 0;
                                m
                            },
                            Err(e) => {
                                app_error!("Failed to dequeue in baseline consumer: {}", e);
                                consecutive_failures += 1;
                                if consecutive_failures >= max_consecutive_failures {
                                    app_error!("Failed to dequeue {} times in a row. Restarting consumer.", max_consecutive_failures);
                                    break;
                                }
                                continue;
                            }
                        };

                        if mints.is_empty() {
                            tokio::time::sleep(Duration::from_millis(100)).await;
                            continue;
                        }

                        app_info!("Dequeued {} mints for baseline processing, memory available: {}/{}",
                            mints.len(),
                            memory_semaphore.available_permits(),
                            max_tasks_in_memory
                        );

                        for (_message_id, mint) in mints {
                            let controller = self.clone();
                            let exec_sem = execution_semaphore.clone();
                            let mem_sem = memory_semaphore.clone();

                            // å…ˆè·å–å†…å­˜permitï¼Œå¦‚æœå†…å­˜ä¸­ä»»åŠ¡æ•°è¾¾åˆ°100ï¼Œä¸»å¾ªç¯ä¼šåœ¨è¿™é‡Œé˜»å¡
                            // å½“æŸä¸ªä»»åŠ¡å®Œæˆæ—¶ï¼Œä¼šé‡Šæ”¾memory_permitï¼Œä¸»å¾ªç¯æ¢å¤
                            let memory_permit = match mem_sem.acquire_owned().await {
                                Ok(p) => p,
                                Err(e) => {
                                    app_error!("Failed to acquire memory semaphore: {}", e);
                                    continue;
                                }
                            };

                            let self_clone = self.clone();
                            tokio::spawn(async move {
                                // æŒæœ‰memory_permitï¼Œä»»åŠ¡ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
                                let _mem_permit = memory_permit;

                                // åœ¨ä»»åŠ¡å†…éƒ¨è·å–æ‰§è¡Œpermitï¼Œä¸é˜»å¡ä¸»å¾ªç¯
                                let exec_permit = match exec_sem.acquire_owned().await {
                                    Ok(p) => p,
                                    Err(e) => {
                                        app_error!("Failed to acquire execution semaphore for mint {}: {}", mint, e);
                                        return;
                                    }
                                };
                                let _exec_permit = exec_permit;

                                // æ ¸å¿ƒå¤„ç†
                                let result = self_clone.process_single_baseline(&mint, false).await;

                                // å¤„ç†å®Œæˆåç«‹å³ACK
                                match result {
                                    Ok(_) => {
                                        // todo!: æš‚æ—¶å¯¹æ²¡æœ‰å¤„ç†çš„å¤§ä»£å¸ä¹Ÿè¿›è¡Œack
                                        if let Err(e) = controller.kafka_queue.ack_baseline_tasks(consumer_name).await {
                                            app_error!("Failed to ACK message {} for mint {}: {}", consumer_name, mint, e);
                                        } else {
                                            app_info!("âœ… Baseline completed for mint: {}", mint);
                                        }
                                    }
                                    Err(e) => {
                                        app_error!("âŒ Baseline failed for mint {}: {}", mint, e);
                                    }
                                }
                                // _mem_permit åœ¨è¿™é‡Œdropï¼Œé‡Šæ”¾å†…å­˜æ§½ä½
                            });
                        }
                    }
                }
            }
            app_error!("Baseline consumer loop failed. Retrying in 10 seconds...");
            tokio::time::sleep(Duration::from_secs(10)).await;
        }

        Ok(())
    }

    /// å¤„ç†å•ä¸ª mint çš„å®Œæ•´ baseline æµç¨‹
    pub async fn process_single_baseline(&self, mint: &str, is_find: bool) -> Result<i64> {
        // å¦‚æœæ•°é‡å¤§äºæŸä¸ªé˜ˆå€¼ï¼Œåˆ™è§†ä¸ºbig tokenï¼Œä½¿ç”¨å…¶ä»–æ–¹å¼è·å–
        let onchain_holder_count = self.http_client.get_sol_scan_holder(mint).await?;
        if onchain_holder_count >= *BIG_TOKEN_HOLDER_COUNT {
            // å¦‚æœæ˜¯å¤§ä»£å¸ï¼Œç›´æ¥è¿”å›solscançš„å€¼
            app_info!("big token:{}, holder count: {}", mint, onchain_holder_count);
            return Ok(onchain_holder_count as i64);
        }

        // æ­¥éª¤ 1: æ„å»º baseline æ•°æ®
        let baseline_slot = match self.build_baseline(mint).await {
            Ok(slot) => {
                app_info!("Baseline data fetched for mint {}, slot: {}", mint, slot);
                slot
            }
            Err(e) => {
                app_error!("Failed to build baseline for mint {}: {}", mint, e);
                return Err(e);
            }
        };

        // æ­¥éª¤ 2: è®°å½• baseline å¼€å§‹çŠ¶æ€
        if let Err(e) = self
            .database
            .start_baseline_batch(&[mint.to_string()], &[baseline_slot])
            .await
        {
            app_error!("Failed to mark baseline start for mint {}: {}", mint, e);
            return Err(e);
        }

        // æ­¥éª¤ 3: æ ‡è®° baseline å®Œæˆï¼Œè¿›å…¥ catching_up çŠ¶æ€
        if let Err(e) = self
            .database
            .finish_baseline_batch(&[mint.to_string()])
            .await
        {
            app_error!("Failed to mark baseline finish for mint {}: {}", mint, e);
            return Err(e);
        }

        // æ­¥éª¤ 4: æ‰§è¡Œ catch-upï¼Œè¿½èµ¶å†å²äº‹ä»¶
        if let Err(e) = self.catch_up(baseline_slot, mint).await {
            app_error!("Failed to catch up for mint {}: {}", mint, e);
            return Err(e);
        }

        // æ­¥éª¤ 5: æ ‡è®° catch-up å®Œæˆï¼Œè¿›å…¥ synced çŠ¶æ€
        if let Err(e) = self
            .database
            .finish_catch_up_batch(&[mint.to_string()])
            .await
        {
            app_error!("Failed to mark catch up finish for mint {}: {}", mint, e);
            return Err(e);
        }

        app_info!("âœ… Full baseline pipeline completed for mint: {}", mint);

        let mut return_count = onchain_holder_count as i64;
        if is_find {
            return_count = self.database.get_holder_account(mint).await?;
        }
        Ok(return_count)
    }

    pub async fn build_baseline(&self, mint: &str) -> Result<i64> {
        app_info!("start building baseline for: {}", mint);
        let token_accounts = self.http_client.get_token_holders(mint).await?;

        let baseline_slot = if !token_accounts.is_empty() {
            // ä½¿ç”¨åŸå­æ€§æ–¹æ³•å»ºç«‹ baselineï¼Œç¡®ä¿ä¸‰å¼ è¡¨åŒæ—¶æˆåŠŸæˆ–åŒæ—¶å¤±è´¥
            self.database
                .establish_baseline_atomic(mint, token_accounts)
                .await?
        } else {
            0
        };

        Ok(baseline_slot)
    }

    /// ä» baseline_slot è¿½èµ¶åˆ°å½“å‰å·²æœ‰çš„å†å²äº‹ä»¶
    /// å½“ next_cursor ä¸º None æ—¶è¡¨ç¤ºå†å²æ•°æ®å·²è¿½å®Œï¼Œç›´æ¥é€€å‡º
    /// ä¹‹åçš„æ–°äº‹ä»¶ç”± consume_events_from_queue ç»Ÿä¸€å¤„ç†
    pub async fn catch_up(&self, baseline_slot: i64, mint: &str) -> Result<()> {
        const BATCH_SIZE: i64 = 1000;
        let mut cursor = (baseline_slot - 1, i64::MAX);
        let mut total_processed = 0;

        app_info!(
            "Starting catch-up for mint {} from slot {}",
            mint, baseline_slot
        );

        // åœ¨å¼€å§‹å¤„ç†ä¹‹å‰ï¼Œå°† baseline_slot ä¹‹å‰çš„æ‰€æœ‰æœªç¡®è®¤äº‹ä»¶æ ‡è®°ä¸º confirmed
        // å› ä¸º baseline å·²ç»ä»£è¡¨äº†é‚£ä¸ªæ—¶åˆ»çš„å®Œæ•´çŠ¶æ€ï¼Œè¿™äº›è¿‡æ—¶çš„äº‹ä»¶ä¸éœ€è¦å†å¤„ç†
        let skipped_count = self
            .clickhouse
            .confirm_events_before_baseline(mint, baseline_slot)
            .await?;
        if skipped_count > 0 {
            app_info!(
                "Skipped {} events before baseline_slot {} for mint {}",
                skipped_count, baseline_slot, mint
            );
        }

        loop {
            match self
                .clickhouse
                .get_next_events_batch(cursor, mint, BATCH_SIZE)
                .await
            {
                Ok((token_events, Some(next_cursor))) => {
                    // æœ‰æ›´å¤šå†å²æ•°æ®ï¼Œç»§ç»­å¤„ç†
                    if token_events.is_empty() {
                        cursor = next_cursor;
                        continue;
                    }

                    app_info!("In next_cursor to sync mint atomic");
                    self.database
                        .upsert_synced_mints_atomic(&token_events, &self.clickhouse)
                        .await?;

                    total_processed += token_events.len();
                    cursor = next_cursor;
                }
                Ok((token_events, None)) => {
                    // æ²¡æœ‰æ›´å¤šå†å²æ•°æ®ï¼Œå¤„ç†æœ€åä¸€æ‰¹åé€€å‡º
                    app_info!("In none to sync mint atomic");
                    if !token_events.is_empty() {
                        self.database
                            .upsert_synced_mints_atomic(&token_events, &self.clickhouse)
                            .await?;
                        total_processed += token_events.len();
                    }

                    app_info!(
                        "Catch-up completed for mint {}: processed {} events",
                        mint, total_processed
                    );
                    break;
                }
                Err(e) => {
                    app_error!("Failed to get events batch for mint {}: {}", mint, e);
                    return Err(e);
                }
            }
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tracing_subscriber::layer::SubscriberExt;
    use tracing_subscriber::util::SubscriberInitExt;
    use tracing_subscriber::{EnvFilter, Layer, fmt};

    fn set_up() {
        dotenv::dotenv().ok();
        let console_subscriber = fmt::layer()
            .with_target(false)
            .with_level(false)
            .with_writer(std::io::stdout);
        tracing_subscriber::registry()
            .with(
                console_subscriber.with_filter(
                    EnvFilter::try_from_default_env().unwrap_or_else(|_| {
                        "info,rustls=warn,sqlx=warn,hyper=warn,tokio=warn".into()
                    }),
                ),
            )
            .init();
    }

    // #[tokio::test]
    // async fn test_consume_baseline_mints_for_queue() {
    //     set_up();
    //     // åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
    //     let redis_url = std::env::var("REDIS_URL");
    //     let config = RedisQueueConfig::default();
    //     let message_queue = Arc::new(Redis::new(&redis_url.unwrap(), config).await.unwrap());
    //     let _ = message_queue.init_baseline_queue().await.unwrap();
    //
    //     let db_url = std::env::var("DATABASE_URL").unwrap();
    //     let database_config = DatabaseConfig::new_optimized(db_url);
    //     let database = Arc::new(DatabaseConnection::new(database_config).await.unwrap());
    //
    //     let http_rpc = std::env::var("RPC_URL").unwrap();
    //     let http_client = Arc::new(HttpClient::default());
    //
    //     let sync_controller =
    //         SyncController::new(message_queue.clone(), database.clone(), http_client.clone());
    //
    //     let cancellation_token = CancellationToken::new();
    //     let token = cancellation_token.child_token();
    //     if let Err(e) = sync_controller
    //         .consume_baseline_mints_for_queue(token)
    //         .await
    //     {
    //         app_error!("Monitor error: {:?}", e);
    //     }
    // }
}